{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Import all the packages\n"
      ],
      "metadata": {
        "id": "MBcBwrBunch6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# drive.mount('/content/gdrive')\n",
        "drive.mount('/content/drive')\n",
        "home_path = Path(\"/content/drive/MyDrive/yandex/project/cpgene\")\n",
        "os.chdir(home_path)\n",
        "\n",
        "print(home_path)\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "1kbjjr3kLCYx",
        "outputId": "36304344-21ad-436a-ddc8-c9278a23245c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-30c7af72d725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# drive.mount('/content/gdrive')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mhome_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/yandex/project/cpgene\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "\n",
        "#import boto3\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "# from learn.dataset import TabularDataset\n",
        "# from learn.model import CiteAutoencoder\n",
        "# from learn.train import train_model, get_encodings\n",
        "\n",
        "!pip install umap-learn\n",
        "!pip install umap-learn[parametric_umap]\n",
        "!pip install umap-learn[plot]\n",
        "import umap\n"
      ],
      "metadata": {
        "id": "drqhuqnlLIAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UNIVERSAL FUNCTIONS"
      ],
      "metadata": {
        "id": "Xf687lq3lfDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Markers"
      ],
      "metadata": {
        "id": "vFmmJx0zeSeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MARKERS\n",
        "markers_all = dict()\n",
        "markers_all = {'immune' : [\"PTPRC\"],\n",
        "'stroma' : [\"FAP\",\"HTRA1\",\"FBN1\"],\n",
        "'epithel' : [\"KRT2\",\"KRT7\",\"KRT8\",\"VIL1\",\"KRT19\",\"KRT20\",\"CLDN7\",\"EPCAM\",\"MUC1\",\"KRT18\",\"CLDN4\"],\n",
        "'melanocyte': [\"MIA\",\"SLC45A2\",\"PMEL\",\"MLANA\"],\n",
        "'keratinocyte': [\"KRT1\",\"KRT2\",\"KRT5\",\"KRT10\",\"KRT14\",\"KRT15\"],\n",
        "'tcell': [\"CD3E\",\"CD3D\",\"CD3G\"],\n",
        "'bcell': [\"FCRL1\",\"FCRLA\",\"PAX5\",\"MS4A1\",\"CD19\",\"CD79B\",\"CLEC17A\",\"CD22\",\"CD79A\"],\n",
        "'myeloid': [\"CCR1\",\"CD68\",\"CD14\",\"CSF1R\",\"CD163\",\"IGSF6\",\"C5AR1\",\"LILRA5\"],\n",
        "'neutrophil': [\"CXCR2\",\"TMEM252\",\"ALPL\",\"KRT23\",\"FCGR3B\",\"FCGR3A\",\"MGAM\",\"CXCR1\",\"HIST2H2AA4\"],\n",
        "'m2': [\"MSR1\",\"CD38\",\"HLA-DRA\",\"HLA-DRB1\",\"HLA-DRB5\",\"MRC1\",\"C1QA\"],\n",
        "'natural_killer': [\"NCAM1\",\"NCR1\",\"KLRB1\",\"FCGR3A\",\"FGFBP2\",\"GZMB\"],\n",
        "'plasma': [\"DERL3\",\"SDC1\",\"PRDX4\",\"SSR4\",\"JCHAIN\",\"FKBP11\"],\n",
        "'cd4': [\"CD4\"],\n",
        "'cd8': [\"CD8A\",\"CD8B\"],\n",
        "'treg': [\"IL2RA\",\"FOXP3\",\"CCR8\"],\n",
        "'naive': [\"TCF7\",\"IL7R\",\"CCR7\",\"SELL\",\"LEF1\",\"LTB\"],\n",
        "'early': [\"GZMK\",\"SH2D1A\",\"EOMES\",\"DTHD1\",\"SLAMF7\",\"FCRL3\",\"CD28\",\"CXCR5\"],\n",
        "'exhausted': [\"HAVCR2\",\"ENTPD1\",\"GZMB\",\"LAYN\",\"CD38\",\"VCAM1\",\"TOX\",\"LAG3\"],\n",
        "'cycling': [\"MKI67\",\"TOP2A\"],\n",
        "'tismem': [\"ITGAE\",\"ZNF683\",\"ITGA1\"],\n",
        "'eff': [\"FCGR3A\",\"FGFBP2\",\"CX3CR1\",\"SORL1\",\"S1PR1\",\"KLF2\",\"KLRG1\",\"LILRB1\"],\n",
        "'caf': [\"S100A4\",\"CD248\",\"PDPN\",\"THY1\",\"COL1A1\",\"COL1A2\",\"COL6A1\",\"COL6A2\",\"COL6A3\",\"DCN\"],\n",
        "'pericyte': [\"CSPG4\",\"RGS5\",\"ACTA2\",\"DES\",\"PDGFRB\"],\n",
        "'endothel': [\"PECAM1\",\"FLT4\",\"STAB1\",\"FLT1\",\"ICAM1\",\"ICAM2\",\"SELE\",\"SELP\",\"VWF\",\"CD34\",\"CDH5\"]}\n"
      ],
      "metadata": {
        "id": "lcvPKfO4rpW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markers_four = dict()\n",
        "markers_four = {'Naive': [\"TCF7\",\"IL7R\",\"CCR7\",\"SELL\",\"LEF1\",\"LTB\"],\n",
        "'EarlyMemory': [\"GZMK\",\"SH2D1A\",\"EOMES\",\"DTHD1\",\"SLAMF7\",\"FCRL3\",\"CD28\",\"CXCR5\"],\n",
        "'Exhausted': [\"HAVCR2\",\"ENTPD1\",\"GZMB\",\"LAYN\",\"CD38\",\"VCAM1\",\"TOX\",\"LAG3\"],\n",
        "'TissueResidentMemory': [\"ITGAE\",\"ZNF683\",\"ITGA1\"],\n",
        "'AbundantGenes':[\"GAPDH\",\"RPL19\",\"ACTB\",\"PTPRC\",\"CD3D\",\"CD3E\",\"CD3G\",\"CD8A\",\"CD8B\"],\n",
        "'CheckPoints':[\"CTLA4\",\"PDCD1\",\"TIGIT\",\"PVRIG\",\"CD96\",\"CD226\"]\n",
        "}"
      ],
      "metadata": {
        "id": "_SSCfCiL5ebN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markers_last = dict()\n",
        "markers_last = {\n",
        "    'Naive': [\"TCF7\",\"IL7R\",\"CCR7\",\"SELL\",\"LEF1\",\"LTB\"],\n",
        "    'EarlyMemory': [\"GZMK\",\"SH2D1A\",\"EOMES\",\"DTHD1\",\"SLAMF7\",\"FCRL3\",\"CXCR5\"],\n",
        "    'Exhausted': [\"HAVCR2\",\"ENTPD1\",\"LAYN\",\"CD38\",\"VCAM1\",\"TOX\",\"LAG3\"],\n",
        "    'TissueResidentMemory': [\"ITGAE\",\"ZNF683\",\"ITGA1\"],\n",
        "    'TEM':[\"FCGR3A\",\"FGFBP2\",\"CX3CR1\",\"LILRB1\",\"SORL1\"],\n",
        "    'Cycling':[\"TOP2A\",\"MKI67\"],\n",
        "    'CirculatingMemory':[\"NR4A1\",\"NR4A2\",\"NR4A3\",\"KLF2\"],\n",
        "    'AbundantGenes':[\"GAPDH\",\"RPL19\",\"ACTB\",\"PTPRC\",\"CD3D\",\"CD3E\",\"CD3G\",\"CD8A\",\"CD8B\"],\n",
        "    'CheckPoints':[\"CTLA4\",\"PDCD1\",\"TIGIT\",\"PVRIG\",\"CD96\",\"CD226\"]\n",
        "    }\n",
        "\n",
        "'''\n",
        "EarlyMemory <- c(\"CXCR5\",\"GZMK\",\"SH2D1A\",\"EOMES\",\"DTHD1\",\"SLAMF7\",\"FCRL3\")  \"CD28\"\n",
        "Exhausted <- c(\"HAVCR2\",\"ENTPD1\",\"LAYN\",\"CD38\",\"LAG3\",\"VCAM1\",\"TOX\")   \"GZMB\"\n",
        "\n",
        "TEM <- c(\"FCGR3A\",\"FGFBP2\",\"CX3CR1\",\"LILRB1\",\"SORL1\")\n",
        "CirculatingMemory <- c(\"NR4A1\",\"NR4A2\",\"NR4A3\",\"KLF2\")\n",
        "\n",
        "# last addition from Yossef (dark gray)\n",
        "AbundantGenes <- c(\"GAPDH\",\"RPL19\",\"ACTB\",\"PTPRC\",\"CD3D\",\"CD3E\",\"CD3G\",\"CD8A\",\"CD8B\")\n",
        "\n",
        "Last confirmation from Yossef:\n",
        "EarlyMemory <- c(\"CXCR5\",\"GZMK\",\"SH2D1A\",\"EOMES\",\"DTHD1\",\"SLAMF7\",\"FCRL3\")\n",
        "\n",
        "Exhausted <- c(\"HAVCR2\",\"ENTPD1\",\"LAYN\",\"CD38\",\"LAG3\",\"VCAM1\",\"TOX\")\n",
        "\n",
        "Naive <- c(\"TCF7\",\"CCR7\",\"IL7R\",\"LEF1\",\"SELL\",\"LTB\")\n",
        "\n",
        "TissueResidentMemory <- c(\"ITGAE\",\"ITGA1\",\"ZNF683\")\n",
        "\n",
        "TEM <- c(\"FCGR3A\",\"FGFBP2\",\"CX3CR1\",\"LILRB1\",\"SORL1\")\n",
        "\n",
        "Cycling <- c(\"TOP2A\",\"MKI67\")\n",
        "\n",
        "CirculatingMemory <- c(\"NR4A1\",\"NR4A2\",\"NR4A3\",\"KLF2\")\n",
        "The abundant genes are just to make sure there is no bias related to gene expression.\n",
        "\n",
        "AbundantGenes <- c(\"GAPDH\",\"RPL19\",\"ACTB\",\"PTPRC\",\"CD3D\",\"CD3E\",\"CD3G\",\"CD8A\",\"CD8B\")\n",
        "\n",
        "\n",
        "The Check Point genes are what we currently studying. They are not a set of cell state specific markers\n",
        "''' "
      ],
      "metadata": {
        "id": "77yRsGWr5sBP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b40e11f1-985b-437a-aaff-70d33763cf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEarlyMemory <- c(\"CXCR5\",\"GZMK\",\"SH2D1A\",\"EOMES\",\"DTHD1\",\"SLAMF7\",\"FCRL3\")  \"CD28\"\\nExhausted <- c(\"HAVCR2\",\"ENTPD1\",\"LAYN\",\"CD38\",\"LAG3\",\"VCAM1\",\"TOX\")   \"GZMB\"\\n\\nTEM <- c(\"FCGR3A\",\"FGFBP2\",\"CX3CR1\",\"LILRB1\",\"SORL1\")\\nCirculatingMemory <- c(\"NR4A1\",\"NR4A2\",\"NR4A3\",\"KLF2\")\\n\\n# last addition from Yossef (dark gray)\\nAbundantGenes <- c(\"GAPDH\",\"RPL19\",\"ACTB\",\"PTPRC\",\"CD3D\",\"CD3E\",\"CD3G\",\"CD8A\",\"CD8B\")\\n\\nLast confirmation from Yossef:\\nEarlyMemory <- c(\"CXCR5\",\"GZMK\",\"SH2D1A\",\"EOMES\",\"DTHD1\",\"SLAMF7\",\"FCRL3\")\\n\\nExhausted <- c(\"HAVCR2\",\"ENTPD1\",\"LAYN\",\"CD38\",\"LAG3\",\"VCAM1\",\"TOX\")\\n\\nNaive <- c(\"TCF7\",\"CCR7\",\"IL7R\",\"LEF1\",\"SELL\",\"LTB\")\\n\\nTissueResidentMemory <- c(\"ITGAE\",\"ITGA1\",\"ZNF683\")\\n\\nTEM <- c(\"FCGR3A\",\"FGFBP2\",\"CX3CR1\",\"LILRB1\",\"SORL1\")\\n\\nCycling <- c(\"TOP2A\",\"MKI67\")\\n\\nCirculatingMemory <- c(\"NR4A1\",\"NR4A2\",\"NR4A3\",\"KLF2\")\\nThe abundant genes are just to make sure there is no bias related to gene expression.\\n\\nAbundantGenes <- c(\"GAPDH\",\"RPL19\",\"ACTB\",\"PTPRC\",\"CD3D\",\"CD3E\",\"CD3G\",\"CD8A\",\"CD8B\")\\n\\n\\nThe Check Point genes are what we currently studying. They are not a set of cell state specific markers\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Upload dataset, filter by cells needed, add markers"
      ],
      "metadata": {
        "id": "A_TpoTVxcxNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_markers(dataset, markers):\n",
        "    count=0\n",
        "    dataset['marker'] = 'none'\n",
        "    print('************************************************')\n",
        "    for key in markers.keys():\n",
        "      count_key=0\n",
        "      #print('Checking markers from set', key)\n",
        "      for each in markers[key]:\n",
        "        if len(dataset.loc[(dataset['gene'] == each)]) >0:\n",
        "          dataset.loc[(dataset['gene'] == each), 'marker'] = key\n",
        "          #print('Found', each)\n",
        "          count+=1\n",
        "          count_key+=1\n",
        "        else:\n",
        "          print('Not found', each)\n",
        "      print(f'Total markers for key {key} is: {count_key}')\n",
        "    print('Total number of markers found', count)\n",
        "    markers_count = dataset['marker'].value_counts()\n",
        "    print('The main markers count ', markers_count[main_markers].sum())\n",
        "    print('The Checkpoints count ', markers_count[check_points].sum())\n",
        "    print('The markers for check purposes count ', markers_count[markers_check].sum())\n",
        "    print('************************************************')\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def filtering_some_genes(dataset):\n",
        "#filtering Mitochondrial and the Mirna genes according to Amit instruction\n",
        "#genes <- colnames(tGSEdata)[-grep(x=colnames(tGSEdata),pattern=\"^R[NUP]|^MT-|^MIR\",perl=T)]\n",
        "\n",
        "#\"^R[NUP]|^MT-|^MIR”\n",
        "#RN\n",
        "#RU\n",
        "#RP\n",
        "#MT- only 1 and not in the beginning \n",
        "#MIR - 1258\n",
        "#343 R[NUP]\n",
        "  print('************************************************')\n",
        "  r = re.compile(\"R[NUP]\")\n",
        "  #excluding R[NUP]\n",
        "  print('The total number of genes filtered out by R[NUP] : ', len(dataset[dataset.gene.str.match(r)]))\n",
        "  if len(dataset[dataset.gene.str.match(r)]) > 0:\n",
        "    dataset = dataset[~dataset.gene.str.match(r)]\n",
        "  r = re.compile(\"MIR\")\n",
        "  print('The total number of genes filtered out by MIR : ', len(dataset[dataset.gene.str.match(r)]))\n",
        "  if len(dataset[dataset.gene.str.match(r)]) > 0:\n",
        "    dataset = dataset[~dataset.gene.str.match(r)]\n",
        "  r = re.compile(\"MT\")\n",
        "  print('The total number of genes filtered out by MT : ', len(dataset[dataset.gene.str.match(r)]))\n",
        "  if len(dataset[dataset.gene.str.match(r)]) > 0:\n",
        "    dataset = dataset[~dataset.gene.str.match(r)]\n",
        "  print('Final dataset shape: ', dataset.shape)\n",
        "  print('************************************************')\n",
        "  return dataset\n",
        "\n",
        "def create_all_markers_ref_set(dataset, markers):\n",
        "    all_markers = []\n",
        "    for key in markers.keys():\n",
        "      all_markers+= markers[key]\n",
        "    all_markers_dataset = dataset.copy()\n",
        "    all_markers_dataset.set_index('gene', inplace=True)\n",
        "    all_indxs = all_markers_dataset.index\n",
        "    to_delete = np.setdiff1d(all_indxs, all_markers)\n",
        "    all_markers_dataset = all_markers_dataset.drop(index=to_delete, axis=0)\n",
        "    #all_markers_dataset\n",
        "    print('Total number of markers used', len(all_markers_dataset))\n",
        "    all_markers_dataset.reset_index(inplace=True)\n",
        "    #print('all_markers_dataset', all_markers_dataset.columns)\n",
        "    \n",
        "    return all_markers_dataset\n",
        "\n",
        "def add_markers(dataset_f, dataset_or, markers):\n",
        "  #function adds markers to the dataset\n",
        "    count=count_tot=0\n",
        "    print('************************************************')\n",
        "    for key in markers.keys():\n",
        "      count_tot+=len(markers[key])\n",
        "      for each in markers[key]:\n",
        "        if len(dataset_f.loc[(dataset_f['gene'] == each)]) == 0:\n",
        "          # if there is no such marker in the final dataset\n",
        "          print('Found gene that was absent. Adding it...', each)\n",
        "          #print(dataset_f.columns)\n",
        "          #print(dataset_or.columns)\n",
        "          dataset_f=dataset_f.append(dataset_or.loc[(dataset_or['gene'] == each)])      \n",
        "          count+=1\n",
        "        else:\n",
        "          #print('Already exists')\n",
        "          pass\n",
        "    print('Total number of markers', count_tot)\n",
        "    print('Total number of markers added', count)\n",
        "    print('************************************************')\n",
        "    dataset_f = dataset_f.sort_index()\n",
        "    return dataset_f\n",
        "\n",
        "def prepare_dataset(home_path, path_dataset, path_labels, dataset_name, markers, cell_name_to_filter=None, filter_mit=True):\n",
        "  #prepares data for the following processing (PCA or NN) \n",
        "  #removes cells that we do not need, filter out genes that we do not need, etc.\n",
        "  #help functions are above\n",
        "    name_file = dataset_name + '.parquet.gzip'\n",
        "    dataset = pd.read_parquet(os.path.join(home_path, name_file))\n",
        "    #dataset = pd.read_parquet(path_dataset)\n",
        "    column_names = dataset.columns\n",
        "    print(column_names[:5])\n",
        "    if 'symbol' in column_names:\n",
        "        dataset.columns = ['gene' if x=='symbol' else x for x in dataset.columns]\n",
        "    print(dataset_name, ' data shape:', dataset.shape)\n",
        "    if not cell_name_to_filter == None:\n",
        "      labels = pd.read_csv(path_labels, '\\t', header=None)\n",
        "      print(dataset_name, ' labels shape:', labels.shape)\n",
        "      print(labels[1].value_counts())\n",
        "      labels = labels.loc[labels[1] == cell_name_to_filter]\n",
        "      labels.drop(1, axis=1, inplace=True)\n",
        "      labels_list = labels.to_numpy().reshape(1, -1)\n",
        "\n",
        "      #all the columns of the whole dataset\n",
        "      columns = dataset.columns[1:-1]\n",
        "      #we have to find the columns that are NOT in CD8 set\n",
        "      col_to_drop = [x for x in columns if x not in labels_list]\n",
        "      #now we remove these columns from the dataset\n",
        "      dataset = dataset.drop(col_to_drop, axis=1)\n",
        "      print('Dataset filtered by cells needed (',cell_name_to_filter, '), final shape:', dataset.shape)\n",
        "\n",
        "    if filter_mit:\n",
        "      print('************************************************')\n",
        "      print('Filtering Mitochondrial and Mirna genes..')\n",
        "      dataset_filtered = filtering_some_genes(dataset)\n",
        "\n",
        "    if markers != None:\n",
        "        print('************************************************')\n",
        "        all_markers_dataset = create_all_markers_ref_set(dataset, markers)\n",
        "        print('Adding markers...')\n",
        "        dataset_filtered = add_markers(dataset_filtered, all_markers_dataset, markers)\n",
        "        dataset_filtered = check_markers(dataset_filtered, markers)\n",
        "        \n",
        "    dataset_filtered.to_parquet('/content/drive/MyDrive/yandex/project/cpgene/data_NSCLC_filtered.gzip', compression='gzip')\n",
        "    name_file = dataset_name + 'filtered_markers.gzip'\n",
        "    dataset_filtered.to_parquet(os.path.join(home_path, name_file), compression='gzip')\n",
        "\n",
        "    return dataset_filtered"
      ],
      "metadata": {
        "id": "Yh0t5x6NlmYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_name\n",
        "# NSCLC_filtered_markers.gzip"
      ],
      "metadata": {
        "id": "ax5KtKdMcujT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preparing data for PCA and running PCA"
      ],
      "metadata": {
        "id": "mnfofa7XWVGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variance_set(X, x_labels):\n",
        "  genes_var = pd.DataFrame(data=X.var(axis=1), columns=['Var'])\n",
        "  print('variance tables shape', genes_var.shape)\n",
        "  print('x_labels', x_labels.shape)\n",
        "  print('x_labels index', x_labels.index)\n",
        "  print('genes_var index', genes_var.index)\n",
        "  print('difference in indexes', np.setdiff1d(x_labels.index, genes_var.index))\n",
        "  print(X.shape)\n",
        "  genes_var['Mean'] = X.mean(axis=1,skipna=True, numeric_only=True)\n",
        "  genes_var['Std'] = X.std(axis=1,skipna=True, numeric_only=True)\n",
        "  print('variance tables shape', genes_var.shape)\n",
        "  genes_var = genes_var.join(x_labels)\n",
        "  print('variance table', genes_var.head())\n",
        "  print('variance tables shape', genes_var.shape)\n",
        "  print('************************************************')\n",
        "  return genes_var\n",
        "'''\n",
        "cnt=0\n",
        "  for key in markers.keys():\n",
        "    for each in markers[key]:\n",
        "      if len(training_set.loc[(training_set['gene'] == each)]) > 0:\n",
        "        cnt+=1\n",
        "  print('Total number of markers equals: ', cnt)\n",
        "\n",
        "'''\n",
        "\n",
        "def check_var_thresh(dataset,th_mean, th_std_ratio):\n",
        "  print('Mean mean value by genes', dataset.Mean.mean(axis=0))\n",
        "  print('Mean std value by genes', dataset.Std.mean(axis=0))\n",
        "  check=dataset[(dataset.Mean>th_mean)&((dataset.Std/dataset.Mean)>th_std_ratio)]\n",
        "  print(check.shape)\n",
        "\n",
        "\n",
        "\n",
        "def prep_data_PCA(home_path, path_dataset, dataset_name, markers, th_mean, th_std_ratio):\n",
        "  #takes the filtered dataset and returns training set for PCA task\n",
        "  name_file = dataset_name + 'filtered_markers.gzip'\n",
        "  dataset_filtered = pd.read_parquet(os.path.join(home_path, name_file)).reset_index(drop=True)\n",
        "  print('Dataset shape: ', dataset_filtered.shape)\n",
        "  x_labels = dataset_filtered['gene']\n",
        "  x_markers = dataset_filtered['marker']\n",
        "  print('x_labels shape', x_labels.shape, 'x_markers shape', x_markers.shape)\n",
        "  X = dataset_filtered.iloc[:, 1:-1]\n",
        "  dataset_filtered['marker'].fillna('none', inplace=True)\n",
        "  print('Markers values: \\n', dataset_filtered['marker'].value_counts())\n",
        "  X = np.log2(X+1)\n",
        "  X['gene'] = x_labels\n",
        "  X['marker'] = x_markers\n",
        "  print('X for PCA shape: ', X.shape)\n",
        "  #calculatin variance for every gene\n",
        "  genes_var = variance_set(X, x_labels)\n",
        "  #filtering genes according to the threshold\n",
        "  training_set = X[(genes_var.Mean>th_mean)&((genes_var.Std/genes_var.Mean)>th_std_ratio)]\n",
        "  print('Training set shape', training_set.shape)\n",
        "  print('Adding markers (check if they were deleted)')\n",
        "  training_set = add_markers(training_set, X, markers)\n",
        "  #print('Final check')\n",
        "  check_markers(training_set, markers)\n",
        "  print('The final dataset shape for PCA: ', training_set.shape)\n",
        "  print('************************************************')\n",
        "  training_set = training_set.sort_index()\n",
        "  return training_set, genes_var \n",
        "\n",
        "\n",
        "def running_PCA(dataset, home_path, num_comp=10):\n",
        "  #we input training_set to run PCA on it (by default using 10 components)\n",
        "  var_pc=np.zeros(num_comp)\n",
        "  gene_labels_fin = dataset['gene'].reset_index(drop=True)\n",
        "  gene_markers_fin = dataset['marker'].reset_index(drop=True).copy()\n",
        "  print('markers check: \\n', gene_markers_fin.value_counts())\n",
        "  tr_x_indxs=dataset.index\n",
        "  X_train = dataset.iloc[:, 1:-2]\n",
        "  print('X_train shape: ', X_train.shape)\n",
        "  print('************************************************')\n",
        "  print(\"Principal Component Analysis (PCA)\")\n",
        "  pca = PCA(n_components = num_comp)\n",
        "  X_pca = pca.fit_transform(X_train)\n",
        "  col_names = list(' ') * num_comp\n",
        "  for i in range(num_comp):\n",
        "    col_names[i] = 'PC' + str(i+1)\n",
        "\n",
        "  #pca_df = pd.DataFrame(data=X_pca, columns=['PC1','PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
        "  pca_df = pd.DataFrame(data=X_pca, columns=col_names)\n",
        "  pca_df = pd.concat([pca_df, gene_labels_fin], axis=1)\n",
        "  pca_df = pd.concat([pca_df, gene_markers_fin], axis=1)\n",
        "  print('explained var by components', np.round(pca.explained_variance_ratio_,5))\n",
        "  for i in range(var_pc.shape[0]):\n",
        "    var_pc[i] = np.round(pca.explained_variance_ratio_*100,5)[i]\n",
        "  #palette = sns.color_palette(\"muted\", n_colors=len(list(labels['cell_type'].unique())))\n",
        "  #pca_df.to_csv('pca_df_10.csv')\n",
        "\n",
        "  #adding date and time to the saved file\n",
        "  name_file= dataset_name +'_' + str(len(markers)) +'_PCA_'+ str(num_comp) + '_' + datetime.now().strftime(\"%d-%m_%H:%M\") + '.csv'\n",
        "  print('Saving results of PCA to csv file...')\n",
        "  pca_df.to_csv(os.path.join(home_path, name_file))\n",
        "  return pca_df, var_pc\n",
        "\n",
        "def plotting_res_sns(df, var_pc, comp1, comp2, color_marker, title):\n",
        "    #takes pca_df and variable with components and the names of the components we are interested in\n",
        "    #comp1 and comp2 are STRINGS in the following format: 'PC1', 'PC2', etc\n",
        "    sns.set_style(\"white\")\n",
        "    indx1 = int(comp1[-1]) - 1 \n",
        "    indx2 = int(comp2[-1]) - 1\n",
        "    #color_marker = {'none':'lightgray', 'Naive': 'blue', 'Exhausted': 'lightgreen', 'TissueResidentMemory': 'cyan', \n",
        "     #                   'EarlyMemory': 'red','TEM': 'purple','CirculatingMemory': 'darkgreen',\n",
        "      #                  'Cycling': 'yellow', 'CheckPoints': 'black', 'AbundantGenes': 'darkgray'}\n",
        "    f, ax = plt.subplots(figsize=(16, 8))\n",
        "    p1 = sns.scatterplot(x=comp1,y=comp2,data=df, linewidth=0.2, s=30, alpha=1, hue='marker', legend='full',\n",
        "                     #palette=['lightgray','black', 'darkgreen', 'purple', 'red', 'blue','lightgreen', 'cyan','darkgray', 'yellow'])\n",
        "                     palette=color_marker)\n",
        "    p1.set_title(title)\n",
        "    plt.xlabel(comp1 + ' - ' + str(var_pc[indx1])[:4] + '%')\n",
        "    plt.ylabel(comp2 + ' - ' + str(var_pc[indx2])[:4] + '%')\n",
        "\n",
        "    #for line in range(0,df.shape[0]):\n",
        "     # if pca_df.PC1[line] > 200:\n",
        "      #  p1.text(pca_df.PC1[line]+0.01, pca_df.PC2[line], \n",
        "       # pca_df.gene[line], horizontalalignment='left', \n",
        "        #size='small', color='black', weight='regular')\n",
        "    #plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plotting_res_plotly(df, var_pc, comp1, comp2, color_marker, title):\n",
        "    indx1 = int(comp1[-1]) - 1 \n",
        "    indx2 = int(comp2[-1]) - 1\n",
        "#color_dict = {0:'blue',1:'red'}\n",
        "#[color_dict[x] for x in Y]\n",
        "#parameter c - defines color for each point\n",
        "#c = [color_dict[x] for x in df.class]\n",
        "\n",
        "    #color_marker = {'none':'lightgray', 'Naive': 'blue', 'Exhausted': 'lightgreen', 'TissueResidentMemory': 'cyan', \n",
        "     #                   'EarlyMemory': 'red','TEM': 'purple','CirculatingMemory': 'darkgreen',\n",
        "      #                  'Cycling': 'yellow', 'CheckPoints': 'black', 'AbundantGenes': 'darkgray'}\n",
        "\n",
        "    fig = px.scatter(df, x=comp1, y=comp2, color=\"marker\", hover_name='gene', #text='gene',\n",
        "                    labels={\n",
        "                        comp1: comp1 + ' - ' + str(var_pc[indx1])[:4] + '%',\n",
        "                        comp2: comp2 + ' - ' + str(var_pc[indx2])[:4] + '%'},\n",
        "                    \n",
        "                    color_discrete_sequence=['lightgray','black', 'darkgreen', 'purple', 'red', 'blue','lightgreen', 'cyan','darkgray', 'yellow'],\n",
        "                    color_discrete_map= color_marker,\n",
        "                    title= title)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def plotting_UMAP_sns(df, comp1, comp2, color_marker, title):\n",
        "    #takes pca_df and variable with components and the names of the components we are interested in\n",
        "    #comp1 and comp2 are STRINGS in the following format: 'PC1', 'PC2', etc\n",
        "    sns.set_style(\"white\")\n",
        "    indx1 = int(comp1[-1]) - 1 \n",
        "    indx2 = int(comp2[-1]) - 1\n",
        "    #color_marker = {'none':'lightgray', 'Naive': 'blue', 'Exhausted': 'lightgreen', 'TissueResidentMemory': 'cyan', \n",
        "     #                   'EarlyMemory': 'red','TEM': 'purple','CirculatingMemory': 'darkgreen',\n",
        "      #                  'Cycling': 'yellow', 'CheckPoints': 'black', 'AbundantGenes': 'darkgray'}\n",
        "    f, ax = plt.subplots(figsize=(16, 8))\n",
        "    p1 = sns.scatterplot(x=comp1,y=comp2,data=df, linewidth=0.2, s=30, alpha=1, hue='marker', legend='full',\n",
        "                     #palette=['lightgray','black', 'darkgreen', 'purple', 'red', 'blue','lightgreen', 'cyan','darkgray', 'yellow'])\n",
        "                     palette=color_marker)\n",
        "    p1.set_title(title)\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.show()\n",
        "\n",
        "'''\n",
        "  fig = px.scatter(plot_emb_res, x=\"UMAP1\", y=\"UMAP2\", color=\"marker\",\n",
        "                  color_discrete_map = color_marker,\n",
        "                  hover_name='gene',\n",
        "                  title = title)                \n",
        "  fig.show()\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def plotting_UMAP_plotly(df, comp1, comp2, color_marker, title):\n",
        "    fig = px.scatter(df, x=comp1, y=comp2, color=\"marker\", hover_name='gene', #text='gene',\n",
        "                    color_discrete_map= color_marker,\n",
        "                    title= title)\n",
        "    fig.show()\n",
        "\n",
        "'''\n",
        "Naive, Exhausted, TissueResidentMemory, EarlyMemory,TEM,CirculatingMemory,Cycling,CheckPoints, Abundant\n",
        "\n",
        "Blue, light green,cyan, red, purple,dark green,yellow,black, dark gray\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0zFwy1m7Vabw",
        "outputId": "e83cc55b-4f01-406e-feb9-421ebc3def83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNaive, Exhausted, TissueResidentMemory, EarlyMemory,TEM,CirculatingMemory,Cycling,CheckPoints, Abundant\\n\\nBlue, light green,cyan, red, purple,dark green,yellow,black, dark gray\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UMAP (embedding and plotting the result)"
      ],
      "metadata": {
        "id": "xAluBaao55sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def umap_embedding(encodings, gene_labels_fin, gene_markers_fin, title):\n",
        "  embedding = umap.UMAP(random_state=0).fit_transform(encodings)\n",
        "  print(embedding.shape)\n",
        "  print(encodings.shape, gene_labels_fin.shape, gene_markers_fin.shape)\n",
        "  plot_emb_res=pd.concat([gene_labels_fin, gene_markers_fin], axis=1)\n",
        "  plot_emb_res[\"UMAP1\"] = embedding[:, 0]\n",
        "  plot_emb_res[\"UMAP2\"] = embedding[:, 1]\n",
        "  #plot_emb_res.head()\n",
        "  plotting_UMAP_plotly(plot_emb_res, 'UMAP1', 'UMAP2', color_marker, title)\n",
        "  plotting_UMAP_sns(plot_emb_res, 'UMAP1', 'UMAP2',color_marker, title)  \n",
        "\n"
      ],
      "metadata": {
        "id": "N200THrk56My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "0dpwqZgYDTYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    \"\"\"Custome dataset for tabular data\"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        return x, x\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_model(model, train_dl, valid_dl, lr, epochs, verbose=True):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n",
        "                                                    steps_per_epoch=len(train_dl), epochs=epochs)\n",
        "    criterion = nn.MSELoss()\n",
        "    history = defaultdict(list)\n",
        "    \n",
        "    mean_losses = []\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        model.train()\n",
        "        \n",
        "        train_loss = 0\n",
        "        nsamples_train = 0\n",
        "        for x, y in train_dl:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            x_prime = model(x.to(device))\n",
        "            \n",
        "            loss = criterion(x_prime, y.to(device))\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # log losses\n",
        "            batch_size = x.shape[0]\n",
        "            nsamples_train += batch_size\n",
        "            train_loss += batch_size*(loss.item())\n",
        "            \n",
        "        valid_loss = 0\n",
        "        nsamples_valid = 0\n",
        "        \n",
        "        model = model.eval()\n",
        "        \n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in valid_dl:\n",
        "                x_prime = model(x.to(device))\n",
        "\n",
        "                loss = criterion(x_prime, y.to(device))\n",
        "                \n",
        "                # log losses\n",
        "                batch_size = x.shape[0]\n",
        "                nsamples_valid += batch_size\n",
        "                valid_loss += batch_size*(loss.item())\n",
        "                \n",
        "        train_loss = train_loss / nsamples_train\n",
        "        valid_loss = valid_loss / nsamples_valid\n",
        "\n",
        "        history['train'].append(train_loss)\n",
        "        history['valid'].append(valid_loss)\n",
        "\n",
        "        \n",
        "        if verbose and epoch%10==0:\n",
        "            print(f'Epoch {epoch}: train loss {train_loss}; valid loss {valid_loss}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def get_encodings(model, dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encodings = [model.encoder(x.to(device)) for x, _ in dl]\n",
        "    return torch.cat(encodings, dim=0)\n",
        "\n",
        "\n",
        "class LinBnDrop(nn.Sequential):\n",
        "    \"\"\"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers, adapted from fastai.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=True):\n",
        "        layers = [nn.BatchNorm1d(n_out if lin_first else n_in)] if bn else []\n",
        "        if p != 0: layers.append(nn.Dropout(p))\n",
        "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
        "        if act is not None: lin.append(act)\n",
        "        layers = lin+layers if lin_first else layers+lin\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder for CITE-seq data\"\"\"\n",
        "    \n",
        "    def __init__(self, nfeatures_rna, nfeatures_pro, hidden_rna, hidden_pro, z_dim):\n",
        "        super().__init__()\n",
        "        self.nfeatures_rna = nfeatures_rna\n",
        "        self.nfeatures_pro = nfeatures_pro\n",
        "\n",
        "        if nfeatures_rna > 0:\n",
        "            self.encoder_rna = LinBnDrop(nfeatures_rna, hidden_rna, p=0.1, act=nn.LeakyReLU())\n",
        "\n",
        "        if nfeatures_pro > 0:\n",
        "            self.encoder_protein = LinBnDrop(nfeatures_pro, hidden_pro, p=0.1, act=nn.LeakyReLU())\n",
        "\n",
        "        # make sure hidden_rna and hidden_pro are set correctly\n",
        "        hidden_rna = 0 if nfeatures_rna == 0 else hidden_rna\n",
        "        hidden_pro = 0 if nfeatures_pro == 0 else hidden_pro\n",
        "        \n",
        "        self.encoder = LinBnDrop(hidden_rna + hidden_pro, z_dim, act=nn.LeakyReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.nfeatures_rna > 0 and self.nfeatures_pro > 0:\n",
        "            x_rna = self.encoder_rna(x[:, :self.nfeatures_rna])\n",
        "            x_pro = self.encoder_protein(x[:, self.nfeatures_rna:])\n",
        "            x = torch.cat([x_rna, x_pro], 1)\n",
        "\n",
        "        elif self.nfeatures_rna > 0 and self.nfeatures_pro == 0:\n",
        "            x = self.encoder_rna(x)\n",
        "\n",
        "        elif self.nfeatures_rna == 0 and self.nfeatures_pro > 0:\n",
        "            x = self.encoder_protein(x)\n",
        "            \n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder for CITE-seq data\"\"\"\n",
        "    def __init__(self, nfeatures_rna, nfeatures_pro, hidden_rna, hidden_pro, z_dim):\n",
        "        super().__init__()\n",
        "        self.nfeatures_rna = nfeatures_rna\n",
        "        self.nfeatures_pro = nfeatures_pro\n",
        "\n",
        "        # make sure hidden_rna and hidden_pro are set correctly\n",
        "        hidden_rna = 0 if nfeatures_rna == 0 else hidden_rna\n",
        "        hidden_pro = 0 if nfeatures_pro == 0 else hidden_pro\n",
        "\n",
        "        hidden = hidden_rna + hidden_pro\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            LinBnDrop(z_dim, hidden, act=nn.LeakyReLU()),\n",
        "            LinBnDrop(hidden, nfeatures_rna + nfeatures_pro, bn=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "        \n",
        "class CiteAutoencoder(nn.Module):\n",
        "    def __init__(self, nfeatures_rna=0, nfeatures_pro=0, hidden_rna=120, hidden_pro=8, z_dim=20):\n",
        "        \"\"\" Autoencoder for citeseq data \"\"\"\n",
        "        super().__init__()\n",
        " \n",
        "        self.encoder = Encoder(nfeatures_rna, nfeatures_pro, hidden_rna, hidden_pro, z_dim)\n",
        "        self.decoder = Decoder(nfeatures_rna, nfeatures_pro, hidden_rna, hidden_pro, z_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "CAc7WYr7DUwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation for the model\n",
        "\n",
        "def data_prep_train_for_NN(training_set, n_dim, n_epochs=15):\n",
        "  gene_labels_fin = training_set['gene'].reset_index(drop=True).copy()\n",
        "  gene_markers_fin = training_set['marker'].reset_index(drop=True).copy()\n",
        "  X_nn = training_set.reset_index(drop=True)\n",
        "  X_nn = X_nn.iloc[:, :-2]\n",
        "  print(f'Shape of the dataset for the modelling: {X_nn.shape}')\n",
        "  train, valid = train_test_split(X_nn.to_numpy(dtype=np.float32), test_size=0.1, random_state=0)\n",
        "  print(f'Shape of the train set: {train.shape}, shape of the validation set {valid.shape}')\n",
        "  nfeatures  = X_nn.shape[1]\n",
        "  #print(f'Number of features for the train: {nfeatures}')\n",
        "  train_ds = TabularDataset(train)\n",
        "  valid_ds = TabularDataset(valid)\n",
        "  train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "  valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=False)\n",
        "  #x, y = next(iter(train_dl))\n",
        "  #x.shape, y.shape\n",
        "  model = CiteAutoencoder(nfeatures_rna=nfeatures, nfeatures_pro=0, hidden_rna=400, hidden_pro=0, z_dim=n_dim)\n",
        "  print(model)\n",
        "  lr = 1e-2\n",
        "  epochs = n_epochs\n",
        "  model, losses = train_model(model, train_dl, valid_dl, lr=lr, epochs=epochs)\n",
        "  fig =  plt.plot(figsize=(15,5))\n",
        "  plt.plot(losses['train'], label = 'Train loss')\n",
        "  plt.plot(losses['valid'], label='Validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  return model, X_nn, gene_labels_fin, gene_markers_fin\n",
        "\n",
        "def test_NN_print_result(model, X_nn, gene_labels_fin, gene_markers_fin, n_dim):\n",
        "  test_ds = TabularDataset(X_nn.to_numpy(dtype=np.float32))\n",
        "  test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "  encodings = get_encodings(model, test_dl)\n",
        "  encodings = encodings.cpu().numpy()\n",
        "  print(f'Encoding shape: {encodings.shape}')\n",
        "  columns = list(range(1, n_dim+1))\n",
        "  encodings_df = pd.DataFrame(data=encodings, columns=columns)\n",
        "  res = pd.concat([encodings_df, gene_labels_fin, gene_markers_fin], axis=1)\n",
        "  print(f'Saving csv file with the results of NN modeling, dimensions {n_dim}, markers {len(markers)}')\n",
        "  name_file= dataset_name +'_' + str(len(markers)) +'_NN_'+ str(n_dim) + '_' + datetime.now().strftime(\"%d-%m_%H:%M\") + '.csv'\n",
        "  res.to_csv(os.path.join(home_path, name_file))\n",
        "\n",
        "  return encodings\n",
        "\n",
        "#def printing_UMAP_NNres(encodings, gene_labels_fin, gene_markers_fin):\n",
        " # return\n"
      ],
      "metadata": {
        "id": "e4VxJk_-ybUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running code on different datasets"
      ],
      "metadata": {
        "id": "3maQ-oWYIf6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Universal pipeline (working on the filtered datasets already)\n",
        "\n"
      ],
      "metadata": {
        "id": "ektYpbsIQQG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Two sets of markers in a row"
      ],
      "metadata": {
        "id": "VmLcMd0ncHSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_run():\n",
        "    print(f'Dataset {dataset_name}. \\nRunning data preparation for {marker} markers')\n",
        "\n",
        "    #BASIC PIPELINE: PREPROCESSING (filtering by some genes), PREPARING FOR PCA\n",
        "    dataset_filtered = prepare_dataset(home_path, path_dataset, path_labels, datasets[index_dataset], markers)\n",
        "    #dataset_filtered.head\n",
        "    training_set, var_df = prep_data_PCA(home_path, path_dataset, dataset_name, markers, thresholds[dataset_name][0], thresholds[dataset_name][1])\n",
        "\n",
        "    n = input('Check the thresholds? Y/N')\n",
        "    if n.lower() == 'y':\n",
        "      #check_var_thresh(var_df,0.849, 1.9)\n",
        "        pass\n",
        "    else:\n",
        "        print(f'Running PCA for {dims} dimensions')\n",
        "        pca_df, var_pc = running_PCA(training_set, home_path, dims[-1])\n",
        "        for dim_size in dims:\n",
        "          pca_to_print=pca_df[pca_df.columns[:dim_size]].copy()\n",
        "          pca_to_print = pd.concat([pca_to_print, pca_df[pca_df.columns[-2:]]], axis=1)\n",
        "          title = str(len(markers)) + ' markers, PCA - ' + str(dim_size) +'components'\n",
        "          plotting_res_sns(pca_to_print, var_pc[:dim_size], 'PC2', 'PC3',color_marker, title)\n",
        "          plotting_res_plotly(pca_df, var_pc, 'PC2', 'PC3',color_marker, title)  \n",
        "\n",
        "        print('Printing UMAP maps... ')\n",
        "        for dim_size in dims:\n",
        "          title = 'UMAP of PCA-' + str(dim_size) + ' results ' + str(len(markers)) + ' markers'\n",
        "          umap_embedding(pca_df[pca_df.columns[:dim_size]], pca_df['gene'], pca_df['marker'], title)\n",
        "\n",
        "\n",
        "        print('Running the Neural Network on the same params')\n",
        "        for dim_size in dims:\n",
        "          model, X_nn, gene_labels_fin, gene_markers_fin = data_prep_train_for_NN(training_set, dim_size, n_epochs=15)\n",
        "          encodings = test_NN_print_result(model, X_nn, gene_labels_fin, gene_markers_fin, dim_size)\n",
        "          title = 'UMAP of NN-' + str(dim_size) + ' results ' + str(len(markers)) + ' markers'\n",
        "          umap_embedding(encodings, gene_labels_fin, gene_markers_fin, title)\n"
      ],
      "metadata": {
        "id": "YDnXXxYUcHSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rei8Tg-4cG93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If only one set of markers needed correct the first variable num_markers"
      ],
      "metadata": {
        "id": "Uq0einfVdaDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_markers = [4, 7]\n",
        "print('Datasets: 0 - NSCLC, 1- CRC1, 2- CRC2, 3- HEPA, 4-MELANOMA')\n",
        "index_dataset = int(input('Input the index of the dataset you are interested in: '))\n",
        "n = int(input('Input how many different dimensions you want to check for PCA and NN (1, 2, 3, 4, 5?): '))\n",
        "dims=np.zeros(n, dtype=int)\n",
        "for i in range(n):\n",
        "  dims[i]=int(input(f'Input the number of dimensions for the {i+1} run (10-50-100?): '))\n",
        "print(f'So, the following dimensions will be checked: ', dims)\n",
        "\n",
        "main_markers = ['EarlyMemory', 'Exhausted','Naive','TissueResidentMemory']\n",
        "check_points = ['CheckPoints']\n",
        "markers_check = ['AbundantGenes']\n",
        "\n",
        "thresholds={'NSCLC':[0.71, 2.1], 'crc1':[0.71, 2.1], 'crc2':[0.8, 1.9], \n",
        "            'hepa':[0.849, 1.9], 'mel' :[0.71, 2.1]}\n",
        "datasets = ['NSCLC', 'crc1', 'crc2', 'hepa', 'mel']\n",
        "datasets = ['NSCLC', 'crc1', 'crc2', 'hepa']\n",
        "dataset_name = datasets[index_dataset]\n",
        "home_path = '/content/drive/MyDrive/yandex/project/cpgene/'\n",
        "\n",
        "path_dataset = '/content/drive/MyDrive/yandex/project/cpgene/' + datasets[index_dataset] + '_filtered.gzip'\n",
        "path_labels = '/content/drive/MyDrive/yandex/project/cpgene/labels_' + datasets[index_dataset]\n",
        "\n",
        "    #cell_name_to_filter = 'TTC'  #dataset is filtered already\n",
        "    #NSCLC thresholds 1.2998407729691086/0.7345407510056047 = 1.76\n",
        "    #CRC2 thresholds 1.346980341580908/0.8146161428779355 = 1.65\n",
        "    #hepa thresholds 1.3892587059716444 / 0.8494661794810161 = 1.63\n",
        "\n",
        "\n",
        "for marker in num_markers:\n",
        "    if marker == 4:\n",
        "      color_marker = {'none':'lightgray', 'Naive': 'blue', 'Exhausted': 'lightgreen', 'TissueResidentMemory': 'cyan', \n",
        "                            'EarlyMemory': 'red', 'CheckPoints': 'black', 'AbundantGenes': 'darkgray'}\n",
        "      markers = markers_four\n",
        "    else:                        \n",
        "      color_marker = {'none':'lightgray', 'Naive': 'blue', 'Exhausted': 'lightgreen', 'TissueResidentMemory': 'cyan', \n",
        "                            'EarlyMemory': 'red','TEM': 'purple','CirculatingMemory': 'darkgreen',\n",
        "                            'Cycling': 'yellow', 'CheckPoints': 'black', 'AbundantGenes': 'darkgray'}\n",
        "      markers = markers_last\n",
        "    pipeline_run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653d231b-5bee-4ef9-b1c9-98107d7e26a4",
        "id": "mi3MYmLVcHSC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets: 0 - NSCLC, 1- CRC1, 2- CRC2, 3- HEPA, 4-MELANOMA\n",
            "Input the index of the dataset you are interested in: 0\n",
            "Input how many different dimensions you want to check for PCA and NN (1, 2, 3, 4, 5?): 1\n",
            "Input the number of dimensions for the 1 run (10-50-100?): 10\n",
            "So, the following dimensions will be checked:  [10]\n",
            "Dataset NSCLC. \n",
            "Running data preparation for 4 markers\n",
            "Index(['symbol', 'NTH10-0616A', 'NTH11-0616A', 'NTH15-0616A', 'NTH17-0616A'], dtype='object')\n",
            "NSCLC  data shape: (23459, 12347)\n",
            "************************************************\n",
            "Filtering Mitochondrial and Mirna genes..\n",
            "************************************************\n",
            "The total number of genes filtered out by R[NUP] :  343\n",
            "The total number of genes filtered out by MIR :  1258\n",
            "The total number of genes filtered out by MT :  93\n",
            "Final dataset shape:  (21765, 12347)\n",
            "************************************************\n",
            "************************************************\n",
            "Total number of markers used 40\n",
            "Adding markers...\n",
            "************************************************\n",
            "Found gene that was absent. Adding it... RPL19\n",
            "Total number of markers 40\n",
            "Total number of markers added 1\n",
            "************************************************\n",
            "************************************************\n",
            "Total markers for key Naive is: 6\n",
            "Total markers for key EarlyMemory is: 8\n",
            "Total markers for key Exhausted is: 8\n",
            "Total markers for key TissueResidentMemory is: 3\n",
            "Total markers for key AbundantGenes is: 9\n",
            "Total markers for key CheckPoints is: 6\n",
            "Total number of markers found 40\n",
            "The main markers count  25\n",
            "The Checkpoints count  6\n",
            "The markers for check purposes count  9\n",
            "************************************************\n",
            "Dataset shape:  (21766, 12348)\n",
            "x_labels shape (21766,) x_markers shape (21766,)\n",
            "Markers values: \n",
            " none                    21726\n",
            "AbundantGenes               9\n",
            "EarlyMemory                 8\n",
            "Exhausted                   8\n",
            "CheckPoints                 6\n",
            "Naive                       6\n",
            "TissueResidentMemory        3\n",
            "Name: marker, dtype: int64\n",
            "X for PCA shape:  (21766, 12348)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To run only 1 dataset / 1 set of markers"
      ],
      "metadata": {
        "id": "0KQz5rsWpz6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first setup cell - we need to choose the index of the dataset we are interested in AND the number of markers (4 or 7)"
      ],
      "metadata": {
        "id": "1BunEl-AWZvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "num_markers = int(input('Input number of markers 4 or 7: '))\n",
        "print('Datasets: 0 - NSCLC, 1- CRC1, 2- CRC2, 3- HEPA, 4-MELANOMA')\n",
        "index_dataset = int(input('Input the index of the dataset you are interested in: '))\n",
        "n = int(input('Input how many different dimensions you want to check for PCA and NN (1, 2, 3, 4, 5?): '))\n",
        "dims=np.zeros(n, dtype=int)\n",
        "for i in range(n):\n",
        "  dims[i]=int(input(f'Input the number of dimensions for the {i+1} run (10-50-100?): '))\n",
        "print(f'So, the following dimensions will be checked: ', dims)"
      ],
      "metadata": {
        "id": "Vriwpr-KYt7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the index of the dataset you need to run the code\n",
        "#index_dataset = 0\n",
        "print(f'Running data preparation for {num_markers} markers')\n",
        "datasets = ['NSCLC', 'crc1', 'crc2', 'hepa', 'mel']\n",
        "dataset_name = datasets[index_dataset]\n",
        "home_path = '/content/gdrive/MyDrive/CPP_data/'\n",
        "\n",
        "path_dataset = '/content/gdrive/MyDrive/CPP_data/data_' + datasets[index_dataset] + '_filtered.gzip'\n",
        "path_labels = '/content/gdrive/MyDrive/CPP_data/labels_' + datasets[index_dataset]\n",
        "\n",
        "#cell_name_to_filter = 'TTC'  #dataset is filtered already\n",
        "main_markers = ['EarlyMemory', 'Exhausted','Naive','TissueResidentMemory']\n",
        "check_points = ['CheckPoints']\n",
        "markers_check = ['AbundantGenes']\n",
        "if num_markers == 4:\n",
        "  color_marker = {'none':'lightgray', 'Naive': 'blue', 'Exhausted': 'lightgreen', 'TissueResidentMemory': 'cyan', \n",
        "                        'EarlyMemory': 'red', 'CheckPoints': 'black', 'AbundantGenes': 'darkgray'}\n",
        "  markers = markers_four\n",
        "else:                        \n",
        "  color_marker = {'none':'lightgray', 'Naive': 'blue', 'Exhausted': 'lightgreen', 'TissueResidentMemory': 'cyan', \n",
        "                        'EarlyMemory': 'red','TEM': 'purple','CirculatingMemory': 'darkgreen',\n",
        "                        'Cycling': 'yellow', 'CheckPoints': 'black', 'AbundantGenes': 'darkgray'}\n",
        "  markers = markers_last\n",
        "\n",
        "thresholds={'NSCLC':[0.71, 2.1], 'crc1':[0.71, 2.1], 'crc2':[0.8, 1.9], \n",
        "            'hepa':[0.849, 1.9], 'mel' :[0.71, 2.1]}\n",
        "#NSCLC thresholds 1.2998407729691086/0.7345407510056047 = 1.76\n",
        "#CRC2 thresholds 1.346980341580908/0.8146161428779355 = 1.65\n",
        "#hepa thresholds 1.3892587059716444 / 0.8494661794810161 = 1.63\n",
        "\n",
        "#BASIC PIPELINE: PREPROCESSING (filtering by some genes), PREPARING FOR PCA\n",
        "dataset_filtered = prepare_dataset(home_path, path_dataset, path_labels, datasets[index_dataset], markers)\n",
        "dataset_filtered.head\n",
        "training_set, var_df = prep_data_PCA(home_path, path_dataset, dataset_name, markers, thresholds[dataset_name][0], thresholds[dataset_name][1])\n",
        "\n",
        "n = input('Check the thresholds? Y/N')\n",
        "if n.lower() == 'y':\n",
        "   #check_var_thresh(var_df,0.849, 1.9)\n",
        "    pass\n",
        "else:\n",
        "    print(f'Running PCA for {dims} dimensions')\n",
        "    pca_df, var_pc = running_PCA(training_set, home_path, dims[-1])\n",
        "    for dim_size in dims:\n",
        "      pca_to_print=pca_df[pca_df.columns[:dim_size]].copy()\n",
        "      pca_to_print = pd.concat([pca_to_print, pca_df[pca_df.columns[-2:]]], axis=1)\n",
        "      title = str(len(markers)) + ' markers, PCA - ' + str(dim_size) +'components'\n",
        "      plotting_res_sns(pca_to_print, var_pc[:dim_size], 'PC2', 'PC3',color_marker, title)\n",
        "      plotting_res_plotly(pca_df, var_pc, 'PC2', 'PC3',color_marker, title)  \n",
        "\n",
        "    print('Printing UMAP maps... ')\n",
        "    for dim_size in dims:\n",
        "      title = 'UMAP of PCA-' + str(dim_size) + ' results ' + str(num_markers) + ' markers'\n",
        "      umap_embedding(pca_df[pca_df.columns[:dim_size]], pca_df['gene'], pca_df['marker'], title)\n",
        "\n",
        "\n",
        "    print('Running the Neural Network on the same params')\n",
        "    for dim_size in dims:\n",
        "      model, X_nn, gene_labels_fin, gene_markers_fin = data_prep_train_for_NN(training_set, dim_size, n_epochs=15)\n",
        "      encodings = test_NN_print_result(model, X_nn, gene_labels_fin, gene_markers_fin, dim_size)\n",
        "      title = 'UMAP of NN-' + str(dim_size) + ' results ' + str(num_markers) + ' markers'\n",
        "      umap_embedding(encodings, gene_labels_fin, gene_markers_fin, title)\n"
      ],
      "metadata": {
        "id": "DdEc6FtRQvEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzMGb1eQUDLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sum by columns check"
      ],
      "metadata": {
        "id": "qo6QjNcf88a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_crc1_filtered = pd.read_parquet('/content/gdrive/MyDrive/CPP_data/data_crc1_filtered.gzip')  \n",
        "data_crc1_filtered.sum(axis=0)"
      ],
      "metadata": {
        "id": "sVNE_-v89EgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_crc2_filtered = pd.read_parquet('/content/gdrive/MyDrive/CPP_data/data_crc2_filtered.gzip')  \n",
        "data_crc2_filtered.sum(axis=0)"
      ],
      "metadata": {
        "id": "I8tUVKO483x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_hepa_filtered = pd.read_parquet('/content/gdrive/MyDrive/CPP_data/data_hepa_filtered.gzip')  \n",
        "data_hepa_filtered.sum(axis=0)"
      ],
      "metadata": {
        "id": "e9MA5HWKyPjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_NSCLC_filtered = pd.read_parquet('/content/gdrive/MyDrive/CPP_data/data_NSCLC_filtered.gzip')  \n",
        "data_NSCLC_filtered.sum(axis=0)"
      ],
      "metadata": {
        "id": "uyuPVPlZ8MIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_mel_filtered = pd.read_parquet('/content/gdrive/MyDrive/CPP_data/data_mel_filtered.gzip')  \n",
        "data_mel_filtered.sum(axis=0)"
      ],
      "metadata": {
        "id": "DlyfSE_O8U8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fjhk-RbxJEz8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}